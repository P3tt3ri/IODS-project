A = matrix(1:6,nrow=2)
A
A = matrix(1:6,nrow=2,byrow=TRUE)
A
B = matrix(1:12,nrow=3,byrow=TRUE)
B
A%*%B
AB = matrix(0,nrow=3,ncol=4)
AB
for(i in 1:2) {
for (j in 1:4) {
for (k in 1:3) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
AB = matrix(0,nrow=2,ncol=4)
for(i in 1:2) {
for (j in 1:4) {
for (k in 1:3) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
A%*%B
dim(A)
dim(A)[1]
AB = matrix(0,nrow=2,ncol=4)
for(i in dim(A)[2]) {
for (j in dim(B)[1]) {
for (k in dim(A)[2]) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
AB = matrix(0,nrow=2,ncol=4)
for(i in dim(A)[1]) {
for (j in dim(B)[2]) {
for (k in dim(A)[2]) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
dim(a)[1]
dim(A)[1]
dim(B)[2]
AB = matrix(0,nrow=2,ncol=4)
for(i in 1:dim(A)[1]) {
for (j in 1:dim(B)[2]) {
for (k in 1:dim(A)[2]) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
AB = matrix(0,nrow=dim(A)[1],ncol=dim(B)[2])
for(i in 1:dim(A)[1]) {
for (j in 1:dim(B)[2]) {
for (k in 1:dim(A)[2]) {
AB[i,j] = AB[i,j] + A[i,k]*B[k,j]
}
}
}
AB
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)
summary(lrn14)
#The dataset has 183 rows and 60 columns and consists of mainly integer variables
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
lrn14$deep
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
lrn14$surf
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
lrn14$stra
lrn14<- filter(lrn14, Points > 0)
summary(lrn14$Points)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)
summary(lrn14)
#The dataset has 183 rows and 60 columns and consists of mainly integer variables
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
lrn14$deep
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header = TRUE)
str(lrn14)
# Filter only needed variables, summing and normalizing point values
library(dplyr)
library(ggplot2)
library(GGally)
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep, surface and strategic learnings and create columns 'deep', 'surf' and 'stra' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# Scale summed variable 'attitude' (total points of 50 to the scale of Likert 1-5) by dividing each number in the column vector by 10
Scaled_Attitude <- lrn14$Attitude / 10
# Overwrite column 'attitude' with scaled values
lrn14$Attitude <- Scaled_Attitude
# choose a handful of columns to keep
keep_columns <- c("gender","Age","Attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# Exclude participants with zero points
learning2014_no_zeros <- filter(learning2014, Points != 0)
str(learning2014_no_zeros)
# Write modified data to a CSV-file
write.csv(learning2014_no_zeros, file = "~/Documents/GitHub/IODS-project/data/learning2014.csv")
write.table(learning2014_no_zeros, file = "~/Documents/GitHub/IODS-project/data/learning2014.txt")
# learning2014_read <- read.csv("~/Documents/GitHub/IODS-project/data/learning2014.csv")
learning2014 <- read.table("~/Documents/GitHub/IODS-project/data/learning2014.txt")
# Part two: Analysis
# Summary of data: Variables and their statistics, dimensions of the table and structure of the data table.
summary(learning2014)
dim(learning2014)
str(learning2014)
# Distributions and correlations
ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# Linear models with different variable combinations
# Dependent variable: Points - Explanatory variables: Attitude, Strategic learning, Surface learning
My_model1 <- lm(learning2014$Points ~ learning2014$Attitude + learning2014$stra + learning2014$surf, data = learning2014)
summary(My_model1)
# Dependent variable: Points - Explanatory variables: Attitude, Strategic learning
My_model2 <- lm(learning2014$Points ~ learning2014$Attitude + learning2014$stra, data = learning2014)
summary(My_model2)
# Dependent variable: Points - Explanatory variables: Attitude, Surface learning
My_model3 <- lm(learning2014$Points ~ learning2014$Attitude + learning2014$surf, data = learning2014)
summary(My_model3)
# Dependent variable: Points - Explanatory variable: Attitude
My_model4 <- lm(learning2014$Points ~ learning2014$Attitude, data = learning2014)
summary(My_model4)
# Dependent variable: Points - Explanatory variables: Attitude, Strategic learning, Gender
My_model5 <- lm(learning2014$Points ~ learning2014$Attitude + learning2014$stra + learning2014$gender, data = learning2014)
summary(My_model5)
# Produce the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.
# Explain the assumptions of the model and interpret the validity of those assumptions based on the diagnostic plots.
par(mfrow = c(2,2))
a <- c(1, 2, 5)
plot(My_model4, which = a)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)
summary(lrn14)
#The dataset has 183 rows and 60 columns and consists of mainly integer variables
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
lrn14$deep
#
X = matrix(2,nrow=2)
X
A = matrix(c(-3,4,1,-3),nrow=2,byrow = TRUE)
A
A%*%X
A2= matrix(c(2018,-3),nrow=2)
A2
A2+A1
X = matrix(2,nrow=2)
A1 = matrix(c(-3,4,1,-3),nrow=2,byrow = TRUE)
A1%*%X
A2= matrix(c(2018,-3),nrow=2)
A2+A1
X = matrix(2,nrow=2)
A1 = matrix(c(-3,4,1,-3),nrow=2,byrow = TRUE)
A1%*%X
A2= matrix(c(2018,-3),nrow=2)
A2+A1%*%X
Y=A2+A1%*%X
cov(Y)
covX=matrix(c(1,0,0,4),byrow = TRUE,nrow=2)
covX
A1%*%covX%*%t(A1)
A1%*%covX
#
A=matrix(c(2,-3,2,4),nrow=2,byrow = TRUE)
A
inv(A)
solve(A)
#
A=matrix(c(2,-3,2,4),nrow=2,byrow = TRUE)
solve(A)
det(A)
A
A'
t(A)
A=matrix(c(1/12,0,0,1/12),nrow=2,byrow = TRUE)
A=matrix(c(2,-3,2,4),nrow=2,byrow = TRUE)
solve(A)
det(A)
B=matrix(c(1/12,0,0,1/12),nrow=2,byrow = TRUE)
B
A%*%B%*%t(A)
A%*%B
(A%*%B)%*%t(A)
13/12
#
A=matrix(c(2,-3,2,4),nrow=2,byrow = TRUE)
det(A)
B=matrix(c(1/12,0,0,1/12),nrow=2,byrow = TRUE)
B
A%*%B%*%t(A)
A%*%B
#
x = runif(1000)
y = runif(1000)
u = 2*x - 3*y
v = 2*x + 4*y
cor(u,v)
x = runif(1000)
y = runif(1000)
u = 2*x - 3*y
v = 2*x + 4*y
cor(u,v)
sumCor = 0
for (i in 1:100) {
x = runif(1000)
y = runif(1000)
u = 2*x - 3*y
v = 2*x + 4*y
sumCor = sumCor + cor(u,v)
}
sumCor/100
sumCor = 0
for (i in 1:1000) {
x = runif(1000)
y = runif(1000)
u = 2*x - 3*y
v = 2*x + 4*y
sumCor = sumCor + cor(u,v)
}
sumCor/100
sumCor = 0
for (i in 1:1000) {
x = runif(1000)
y = runif(1000)
u = 2*x - 3*y
v = 2*x + 4*y
sumCor = sumCor + cor(u,v)
}
sumCor/1000
x = runif(100000)
y = runif(100000)
u = 2*x - 3*y
v = 2*x + 4*y
cor(u,v)
x = runif(1000000)
y = runif(1000000)
u = 2*x - 3*y
v = 2*x + 4*y
cor(u,v)
setwd("~/GitHub/IODS-project/data")
library(MASS)
data(Boston)
summary(Boston)
summary(Boston$black)
boston_scaled <- as.data.frame(scale(Boston))
summary(boston_scaled)
quantile(boston_scaled$crim)
View(Boston)
bins <- quantile(boston_scaled$crim)
labs = c("low","med_low","med_high","high")
boston_scaled$crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE,labels=labs)
boston_scaled <- dplyr::select(boston_scaled, -crim)
table(boston_scaled$crime)
?table
ind <- sample(n,  size = nrow(Boston) * 0.8)
ind <- sample(nrow(Boston),  size = nrow(Boston) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
table(train$crime)
table(test$crime)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2,col=classes,pch=classes)
lda.arrows(lda.fit, myscale = 0.3)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2,col=classes,pch=classes)
# lda.arrows(lda.fit, myscale = 0.3)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2,col=classes,pch=classes)
lda.arrows(lda.fit, myscale = 0.3)
confu = table(correct = correct_classes, predicted = lda.pred$class)
correct_classes = test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
confu = table(correct = correct_classes, predicted = lda.pred$class)
confu
diag(confu)
sum(diag(confu))
sum(diag(confu))/nrow(test)
test <- boston_scaled[-ind,]
lda.pred$class
class(lda.pred$class)
as.numeric(class(lda.pred$class))
as.numeric(lda.pred$class)
abs(-3)
sum(abs(correct_classes-lda.pred$class))>1)
sum(abs(correct_classes-lda.pred$class)>1)
correct = as.numeric(correct_classes)
predicted = as.numeric(lda.pred$class)
sum(abs(correct-predicted)>1)
sum(abs(correct-predicted)=0)
sum(abs(correct-predicted)==0)
sum(abs(correct-predicted)>1)
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled <- as.data.frame(scale(Boston))
dist_eu <- dist(Boston)
summary(dist_eu)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <-kmeans(boston_scaled, centers = 8)
pairs(boston_scaled, col = km$cluster)
# plot the Boston dataset with clusters
#pairs(Boston, col = km$cluster)
plot(km)
#pairs(boston_scaled, col = km$cluster)
plot(km)
km <-kmeans(boston_scaled, centers = 8)
#pairs(boston_scaled, col = km$cluster)
plot(km)
#pairs(boston_scaled, col = km$cluster)
plot(km,boston_scaled)
pairs(boston_scaled[6:10], col = km$cluster)
# A short data wrangling script for the IODS course RStudio Exercise 4.
#
# I'm in the habit of setting the working directory excplicitly. If you need to do the same, uncomment the next line.
setwd("~/GitHub/IODS-project/data")
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
View(gii)
str(hd)
summary(hd)
str(gii)
summary(gii)
View(hd)
library(dplyr)
rename(hd, HDI.Rank = hdi)
rename(hd, hdi = HDI.Rank)
gii2 <- gii %>% rename(hdi=HDI.Rank)
hd2 <- hd %>% rename(hdi=HDI.Rank)
hd2 <- hd %>% rename(hdi=HDI.Rank,country=Country,index=Human.Development.Index..HDI.,
lifeExp=Life.Expectancy.at.Birth,expYersOfEdu=Expected.Years.of.Education,
meanYearsOfEdu=Mean.Years.of.Education,gni=GNI.per.Capita.Rank.Minus.HDI.Rank,
gniRankMinusHdiRank = GNI.per.Capita.Rank.Minus.HDI.Rank)
hd2 <- hd %>% rename(hdi=HDI.Rank,country=Country,index=Human.Development.Index..HDI.,
lifeExp=Life.Expectancy.at.Birth,expYersOfEdu=Expected.Years.of.Education,
meanYearsOfEdu=Mean.Years.of.Education, gni=Gross.National.Income..GNI..per.Capita,
gniRankMinusHdiRank = GNI.per.Capita.Rank.Minus.HDI.Rank)
hd2 <- hd %>% rename(hdiRank=HDI.Rank,country=Country,index=Human.Development.Index..HDI.,
lifeExp=Life.Expectancy.at.Birth,expYersOfEdu=Expected.Years.of.Education,
meanYearsOfEdu=Mean.Years.of.Education, gni=Gross.National.Income..GNI..per.Capita,
gniRankMinusHdiRank = GNI.per.Capita.Rank.Minus.HDI.Rank)
gii2 <- gii %>% rename(giiRank=GII.Rank,country=Country,inequalityIndex=Gender.Inequality.Index..GII.,
matMort=Maternal.Mortality.Ratio,adolBirth=Adolescent.Birth.Rate,
parlamentRatio=Percent.Representation.in.Parliament,
secondEduFem=Population.with.Secondary.Education..Female.,
secondEduMale=Population.with.Secondary.Education..Male.,
labourPartFem=Labour.Force.Participation.Rate..Female.,
labourPartMale=Labour.Force.Participation.Rate..Male.)
gii2$eduRatio = gii2$secondEduFem/gii2$secondEduMale
gii2$eduRatio = gii2$secondEduFem/gii2$secondEduMale
gii2$labourRatio = gii2$labourPartFem/gii2$labourPartMale
human=inner_join(hd2,gii2,by=country)
human=inner_join(hd2,gii2,by="country")
View(human)
write.csv2(human,"~/GitHub/IODS-project/data/human.csv",row.names=FALSE)
source('~/GitHub/IODS-project/data/create_human.R')
